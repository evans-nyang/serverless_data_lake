{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Data Lake Immersion\n",
    "## Lab 2.3 - Advanced Data Preparation with AWS Glue Interactive Sessions\n",
    "This example shows how to do joins and filters with transforms on DynamicFrames.\n",
    "For purposes of this Notebook, you need to have done the previous Labs (you should have a Database called `sdl-demo-data` your AWS Glue Data Catalog) as described in the lab guide.\n",
    "### Getting started\n",
    "**Important:** Before running the next step, update the *account_number* variable with your AWS Account Number (e.g. 0123456789112 for the Amazon S3 bucket called 'sdl-immersion-day--0123456789112'\n",
    "\n",
    "DataFrames APIs support elaborate methods for slicing-and-dicing data. This includes operations such as \"selecting\" rows, columns, and cells by name or by number, filtering out rows, etc. Statistical data is usually very messy and contains lots of missing and incorrect values and range violations. Therefore, a critically important feature of DataFrames is the explicit management of missing data.\n",
    "We will write a script that:\n",
    "1. Queries data\n",
    "2. Reformats data\n",
    "3. Repartitions the data\n",
    "\n",
    "Begin by running some boilerplate to import the AWS Glue libraries we'll need and set up a single `GlueContext`.\n",
    "Then, start a Spark application and create dynamic frame from our the data in Amazon S3.\n",
    "Some concepts:\n",
    "- Spark provides a unified platform for writing big data applications, ranging from simple data loading and SQL queries to machine learning and streaming computation over the same engine and with a consistent set of APIs.\n",
    "- Spark handles loading data from Amazon S3.\n",
    "- You control your Spark Application through a driver process called the SparkSession.\n",
    "- A Spark DataFrame is the most common Structured API and simply represents a table of data with rows and columns. (Not to be confused with R and Python DataFrames. Those (with some exceptions) exist on one machine rather than multiple machines)\n",
    "- A schema defines the columns and data types within those columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "account_number = \"ADD_YOUR_AWS_ACCOUNT_NUMBER_HERE\" # <-- Add your aws account number here!\n",
    "\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
    "                            database = \"sdl-demo-data\", \n",
    "                            table_name = \"raw\", \n",
    "                            transformation_ctx = \"datasource0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema of the Dataset\n",
    "Next, you can easily examine the schemas that the crawler recorded in the Data Catalog. For example, to see the schema of the `raw` table, run the following code.\n",
    "\n",
    "**Note:** To have a look at the schema, i.e. the structure of the DataFrame, we'll use the *printSchema* method. This will give us the different columns in our DataFrame, along with the data type and the nullable conditions for that particular column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  10100\n",
      "root\n",
      " |-- productName: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- imageUrl: string (nullable = true)\n",
      " |-- dateSoldSince: string (nullable = true)\n",
      " |-- dateSoldUntil: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- partition_0: string (nullable = true)\n",
      " |-- partition_1: string (nullable = true)\n",
      " |-- partition_2: string (nullable = true)\n",
      " |-- partition_3: string (nullable = true)\n",
      "\n",
      "+--------------------+-------+----------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+-----------+\n",
      "|         productName|  color|department| product|            imageUrl|       dateSoldSince|       dateSoldUntil|price|   campaign|partition_0|partition_1|partition_2|partition_3|\n",
      "+--------------------+-------+----------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+-----------+\n",
      "|Awesome Concrete ...| orange|      Toys|   Chair|http://lorempixel...|Thu Oct 08 2020 0...|Sun Mar 20 2022 1...|  136|       NONE|       2021|         06|         08|         13|\n",
      "|Handmade Granite ...|    tan|    Garden|     Hat|http://lorempixel...|Fri Oct 30 2020 1...|Sat Feb 05 2022 0...|   85|       NONE|       2021|         06|         08|         13|\n",
      "|Gorgeous Metal Sa...|magenta|  Clothing|   Shoes|http://lorempixel...|Fri Oct 16 2020 1...|Fri Sep 03 2021 1...|  138|BlackFriday|       2021|         06|         08|         13|\n",
      "|Unbranded Plastic...|  black|     Music|Computer|http://lorempixel...|Thu Jul 09 2020 0...|Tue Jun 15 2021 2...|   26|BlackFriday|       2021|         06|         08|         13|\n",
      "|  Rustic Steel Pizza|   grey|    Sports|   Mouse|http://lorempixel...|Thu Jan 07 2021 2...|Wed Dec 15 2021 1...|  100|  10Percent|       2021|         06|         08|         13|\n",
      "+--------------------+-------+----------+--------+--------------------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "print (\"Count: \", datasource0.count())\n",
    "\n",
    "df = datasource0.toDF()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Multiple Columns & Filtering Data\n",
    "We can filter our data based on multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----------+-----+-----------+\n",
      "|         productName| product| department|price|   campaign|\n",
      "+--------------------+--------+-----------+-----+-----------+\n",
      "|Unbranded Wooden ...|    Fish|  Computers|   55|BlackFriday|\n",
      "|Unbranded Frozen Hat|    Tuna| Automotive|   85|BlackFriday|\n",
      "|Sleek Metal Sausages|    Ball|   Jewelery|   57|BlackFriday|\n",
      "|  Tasty Plastic Soap|   Chips|Electronics|   19|BlackFriday|\n",
      "|Awesome Cotton Ch...|     Hat|      Tools|  138|BlackFriday|\n",
      "|Handcrafted Fresh...|   Chair|     Sports|   51|BlackFriday|\n",
      "|Gorgeous Plastic ...|   Chips|       Toys|   26|BlackFriday|\n",
      "|Practical Plastic...|Keyboard|   Outdoors|   28|BlackFriday|\n",
      "|    Tasty Soft Chips|    Fish| Industrial|   33|BlackFriday|\n",
      "|Awesome Concrete ...|Computer|       Toys|   35|BlackFriday|\n",
      "+--------------------+--------+-----------+-----+-----------+"
     ]
    }
   ],
   "source": [
    "df.filter((df.campaign=='BlackFriday')).select('productName','product', 'department', 'price','campaign').limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform transformations on data\n",
    "\n",
    "You can easily transform data.\n",
    "\n",
    "Let's only keep the fields that we want and rename `imageUrl` to `thumbnailImageUrl`. The dataset is small enough that we can look at the whole thing. The `toDF()` converts a DynamicFrame to a Spark DataFrame, so we can apply the\n",
    "transforms in SparkSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productName: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- dateSoldSince: string (nullable = true)\n",
      " |-- dateSoldUntil: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- partition_0: string (nullable = true)\n",
      " |-- partition_1: string (nullable = true)\n",
      " |-- partition_2: string (nullable = true)\n",
      " |-- partition_3: string (nullable = true)\n",
      " |-- thumbnailImageUrl: string (nullable = true)\n",
      " |-- campaignType: string (nullable = true)\n",
      "\n",
      "+--------------------+----------+--------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+--------------------+------------+\n",
      "|         productName|department| product|       dateSoldSince|       dateSoldUntil|price|partition_0|partition_1|partition_2|partition_3|   thumbnailImageUrl|campaignType|\n",
      "+--------------------+----------+--------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+--------------------+------------+\n",
      "|Awesome Concrete ...|      Toys|   Chair|Thu Oct 08 2020 0...|Sun Mar 20 2022 1...|  136|       2021|         06|         08|         13|http://lorempixel...|        NONE|\n",
      "|Handmade Granite ...|    Garden|     Hat|Fri Oct 30 2020 1...|Sat Feb 05 2022 0...|   85|       2021|         06|         08|         13|http://lorempixel...|        NONE|\n",
      "|Gorgeous Metal Sa...|  Clothing|   Shoes|Fri Oct 16 2020 1...|Fri Sep 03 2021 1...|  138|       2021|         06|         08|         13|http://lorempixel...| BlackFriday|\n",
      "|Unbranded Plastic...|     Music|Computer|Thu Jul 09 2020 0...|Tue Jun 15 2021 2...|   26|       2021|         06|         08|         13|http://lorempixel...| BlackFriday|\n",
      "|  Rustic Steel Pizza|    Sports|   Mouse|Thu Jan 07 2021 2...|Wed Dec 15 2021 1...|  100|       2021|         06|         08|         13|http://lorempixel...|   10Percent|\n",
      "|Fantastic Plastic...|      Kids|Keyboard|Wed May 05 2021 0...|Thu Oct 28 2021 0...|  135|       2021|         06|         08|         13|http://lorempixel...|        NONE|\n",
      "|   Rustic Soft Mouse|     Music|   Chair|Fri Feb 12 2021 1...|Fri Aug 27 2021 2...|   39|       2021|         06|         08|         13|http://lorempixel...|   10Percent|\n",
      "|Practical Rubber ...| Computers|   Chair|Tue Jan 05 2021 2...|Thu Jul 15 2021 1...|   60|       2021|         06|         08|         13|http://lorempixel...| BlackFriday|\n",
      "|Sleek Concrete Table|    Sports|   Pizza|Fri Nov 06 2020 1...|Tue Nov 30 2021 1...|   27|       2021|         06|         08|         13|http://lorempixel...| BlackFriday|\n",
      "|Intelligent Soft ...|    Sports| Chicken|Wed Feb 17 2021 1...|Fri May 06 2022 1...|   84|       2021|         06|         08|         13|http://lorempixel...|        NONE|\n",
      "+--------------------+----------+--------+--------------------+--------------------+-----+-----------+-----------+-----------+-----------+--------------------+------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "dsTransformed = datasource0.drop_fields(['color','hour']).rename_field('imageUrl', 'thumbnailImageUrl').rename_field('campaign', 'campaignType')\n",
    "dfTransformed = dsTransformed.toDF()\n",
    "\n",
    "dfTransformed.printSchema()\n",
    "\n",
    "dfTransformed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the transformed data to Amazon S3\n",
    "Let's export the transformed dataset in the previous section to Amazon S3. Convert to Parquet format. The following call writes the table across multiple files to support fast parallel reads when doing analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<awsglue.dynamicframe.DynamicFrame object at 0x7ff3dc08b828>"
     ]
    }
   ],
   "source": [
    "glueContext.write_dynamic_frame.from_options(frame = dsTransformed,\n",
    "              connection_type = \"s3\",\n",
    "              connection_options = {\"path\": \"s3a://sdl-immersion-day-\" + account_number + \"/output-etl-nb-jobs\"},\n",
    "              format = \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When execution is finished, go the the Amazon S3 folder, and verify that the files are written. For instance, the folder should look something like:\n",
    "\n`2021-06-15 14:30:01      87705 part-00000-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2021-06-15 14:30:01      88180 part-00001-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2021-06-15 14:30:01      87545 part-00002-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2021-06-15 14:30:01      87705 part-00003-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2021-06-15 14:30:01      88180 part-00004-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet\n",
    "2021-06-15 14:30:02      87545 part-00005-3944ffa1-8917-42f0-93f2-bef5b3c63cca-c000.snappy.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition Data\n",
    "**Important:** Before running the cell below, make sure you are using the correct Amazon S3 path.\n",
    "\n",
    "In the previous example, the data was exported to multiple Amazon S3 objects in parquet format. Since the data is small, let's combine them in a single partition.\n",
    "#### Combine into a Single Partition\n",
    "To put all the history data into a single file, we need to convert it to a data frame, repartition it, and\n",
    "write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfSinglePartition = dfTransformed.repartition(1)\n",
    "dfSinglePartition.write.parquet('s3://sdl-immersion-day-' + account_number + '/output-etl-nb-jobs/singlePartition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When execution is finished, go the the Amazon S3 folder, and verify that the files are written. For instance: the folder should look something like:\n",
    "\n`2021-06-15 14:30:05    1435146 part-00000-95ad4fb6-d178-47ad-8072-d60d8d8e71fd-c000.snappy.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition Based on a Field\n",
    "Or if you want to separate it by the  `department`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfTransformed.write.parquet(\n",
    "        's3a://sdl-immersion-day-' + account_number + '/output-etl-nb-jobs/byDepartment', \n",
    "        partitionBy=['department'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "Many other types of transformations could be done, such as joining tables. AWS Glue makes it easy to write it to relational databases like Amazon Redshift even with semi-structured data. It offers a transform, relationalize(), that flattens DynamicFrames no matter how complex the objects in the frame may be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "Great! We now have the final table that we'd like to use for analysis in Amazon S3, the storage layer of our Data Lake in a compact, efficient format for analytics, that we can run SQL over in AWS Glue, Amazon Athena, or Amazon Redshift Spectrum.\n",
    " \n",
    "Note that, many other types of transformations could be done (e.g. JOIN operations). We leave it to your imagination :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "You've Finished this lab.\n",
    "\n",
    "If you want, you can click the **Save** button at the top of this notebook and safe this as an AWS Glue Job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
